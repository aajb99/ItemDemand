refract <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(refract)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.77
sigma2 <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(refract) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((refract - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
#add prior
curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topleft", c("Prior", "Posterior"), lty=c(2, 1))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
#add prior
curve(dinvgamma(x, gamma, phi), add=T, lty=2)
#add legend
legend("topright", c("Prior", "Posterior"), lty=c(2,1))
# # PRIOR PARAMETERS
# # Prior parameters for mu:
# lambda <- 0.7
# tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
# gamma <- 2.001
# phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
refract <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(refract)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.77
sigma2 <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(refract) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((refract - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
#add prior
curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topleft", c("Prior", "Posterior"), lty=c(2, 1))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# # PRIOR PARAMETERS
# # Prior parameters for mu:
# lambda <- 0.7
# tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
# gamma <- 2.001
# phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
refract <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(refract)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.77
sigma2 <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(refract) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((refract - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topleft", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# # PRIOR PARAMETERS
# # Prior parameters for mu:
# lambda <- 0.7
# tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
# gamma <- 2.001
# phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
refract <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(refract)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.77
sigma2 <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(refract) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((refract - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.77
sigma2 <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2*lambda)/(tau2*n2 + sigma2)
tau2.p <- sigma2*tau2/(tau2*n2 + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu_ea <- 0.77
sigma2_ea <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu_ea), ylab="density", main=expression(pi(mu_ea~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu_ea <- 0.77
sigma2_ea <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu_ea), ylab="density", main=expression(pi(mu_ea~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# Posterior Dist of the difference between WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[WE]-mu[EA]), main="Posterior of "
+ expression(mu[WE]) + " - " + expression(mu[EA]))
# Posterior Dist of the difference between WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[WE]-mu[EA]), main=expression(~"Posterior of " ~mu[WE] ~" - " ~mu[EA]))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
#Given our data and prior knowledge, there is a 95% chance that the
#NYTimes bestseller Fiction goodreads ratings are between 0.3 points
#lower and 0.18 points higher than the nonfiction books
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
setwd("~/byu_fall_2023/Stat_348/STAT348/ItemDemand")
library(timetk)
library(gridExtra)
library(ggplot2)
data_train <- vroom("./data/train.csv")
library(forecast)
storeItem <- data_train %>%
filter(store==9, item==36) %>%
select(date, sales)
rFormula <- sales ~ .
arima_recipe <- recipe(rFormula, data = storeItem) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(arima_recipe) # preprocessing new data
baked_data_arima <- bake(prepped_recipe, new_data = storeItem)
arima_model <- arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
