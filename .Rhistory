# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu[E]), ylab="density", main=expression(pi(mu[E]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
#legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma[E]^2), main=expression(pi(sigma[E]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
#legend("topright", c("Posterior"))
# Posterior Dist of the difference between mean AUROC for WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[W]-mu[E]), main=expression(~"Posterior of " ~mu[W] ~" - " ~mu[E]))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# Posterior Dist of the difference between variance of AUROC for WE and EA:
# Credible interval
diff <- sigma2.use - sigma2_ea.use
plot(density(diff), xlab=expression(sigma[W]^2-sigma[E]^2), main=expression(~"Posterior of " ~sigma[W]^2 ~" - " ~sigma[E]^2))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# WE:
# # PRIOR PARAMETERS
# # Prior parameters for mu:
lambda <- 0.7
tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
gamma <- 2.001
phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
data1 <- c( 0.68, 0.71, 0.71, 0.73, 0.74, 0.74, 0.74, 0.75, 0.75, 0.75, 0.76,
0.76, 0.76, 0.76, 0.76, 0.77, 0.77, 0.78, 0.79, 0.79, 0.79, 0.8,
0.82, 0.82, 0.82, 0.83, 0.83, 0.83, 0.83, 0.84, 0.84, 0.84, 0.84,
0.84, 0.85, 0.85, 0.86, 0.87, 0.87, 0.89)
n <- length(data1)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.79
sigma2 <- 0.04^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
##############################
library(ggplot2)
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
# Import Dataset:
data_train <- vroom("./data/train.csv")
cor_matrix <- cor(data_train)
cor_matrix
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green')),
margins = c(5, 5))
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5))
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
cor_vals_foresttype <- cor_matrix[, 'Cover_Type']
cor_vals_foresttype
cor_vals_foresttype <- cor_matrix[, 'Cover_Type']
ordered_cor_vals_foresttype <- order(-cor_vals_foresttype)
ordered_cor_vals_foresttype
cor_vals_foresttype <- cor_matrix[, 'Cover_Type']
ordered_indeces <- order(-cor_vals_foresttype)
ordered_cor_vals_foresttype <- cor_vals_foresttype[ordered_indeces]
ordered_cor_vals_foresttype
setwd("~/byu_fall_2023/Stat_348/STAT348/AmazonEmployeeAccess")
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(ACTION))# grab training data
library(ggplot2)
boxplot(data_train$ROLE_CODE ~ data_train$ACTION,
col='steelblue',
main='action by role code',
xlab='Action',
ylab='ROLE_CODE')
boxplot(data_train$ROLE_TITLE ~ data_train$ACTION,
col='steelblue',
main='action by role title',
xlab='Action',
ylab='ROLE_TITLE')
boxplot(data_train$ROLE_CODE ~ data_train$ACTION,
col='steelblue',
main='action by role code',
xlab='Action',
ylab='ROLE_CODE')
boxplot(data_train$ROLE_TITLE ~ data_train$ACTION,
col='steelblue',
main='action by role title',
xlab='Action',
ylab='ROLE_TITLE')
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
data_train <- vroom("./data/train.csv") %>%
mutate(ACTION=factor(Cover_Type))# grab training data
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
cor_vals_foresttype <- cor_matrix[, 'Cover_Type']
ordered_indeces <- order(-cor_vals_foresttype)
ordered_cor_vals_foresttype <- cor_vals_foresttype[ordered_indeces]
ordered_cor_vals_foresttype
boxplot(data_train$Soil_Type10 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by soil 10',
xlab='Cover Type',
ylab='Soil 10')
boxplot(data_train$Soil_Type1 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by soil 1',
xlab='Cover Type',
ylab='Soil 1')
boxplot(data_train$Soil_Type22 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by soil 22',
xlab='Cover Type',
ylab='Soil 22')
boxplot(data_train$Soil_Type29 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by soil 29',
xlab='Cover Type',
ylab='Soil 29')
boxplot(data_train$Wilderness_Area1 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by WA 1',
xlab='Cover Type',
ylab='WA 1')
boxplot(data_train$Wilderness_Area3 ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by WA 3',
xlab='Cover Type',
ylab='WA 3')
rFormula <- Cover_Type ~ .
data_train
ordered_cor_vals_foresttype
boxplot(data_train$Hillshade_Noon ~ data_train$Cover_Type,
col='steelblue',
main='Cover type by noon shade',
xlab='Cover Type',
ylab='noon shade')
data_train
data_train[1]
ncols(data_train)
ncol(data_train)
data_train[56]
data_train[55]
data_train[16]
rFormula <- Cover_Type ~ .
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(vars(16:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(Cover_Type)) %>% # get hours
#step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
prepped_recipe
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
class_rf_recipe
cols(data_train)
data_train
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(vars(16:55), fn = factor)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(vars(num_range('16:55')), fn = factor)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(16:55), fn = factor)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
data_train[12]
rFormula <- Cover_Type ~ .
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(Cover_Type)) %>% # get hours
#step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_filter(all_nominal_predictors(), ~nlevels(.) > 1) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(Cover_Type)) %>% # get hours
#step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
setwd("~/byu_fall_2023/Stat_348/STAT348/ItemDemand")
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
data_train <- vroom("./data/train.csv")
nStores <- max(data_train$store)
nStores
nItems <- max(data_train$item)
nItems
data_test <- vroom("./data/test.csv")
library(forecast)
library(modeltime)
library(prophet)
rFormula <- sales ~ .
nStores <- max(data_train$store)
nItems <- max(data_train$item)
library(forecast)
library(modeltime)
library(prophet)
data_test <- vroom("./data/test.csv")
rFormula <- sales ~ .
nStores <- max(data_train$store)
nItems <- max(data_train$item)
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- data_train %>%
filter(store==s, item==i)
storeItemTest <- data_test %>%
filter(store==s, item==i)
## Fit storeItem models here
prophet_recipe <- recipe(rFormula, data = storeItemTrain) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(prophet_recipe) # preprocessing new data
baked_data_prophet <- bake(prepped_recipe, new_data = storeItemTrain)
cv_split_1 <- time_series_split(storeItemTrain, assess="3 months", cumulative = TRUE)
cv_split_1 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_model_1 <- prophet_reg() %>%
set_engine(engine = "prophet") %>%
fit(sales ~ date, data = training(cv_split_1))
cv_results_1 <- modeltime_calibrate(prophet_model_1,
new_data = testing(cv_split_1))
## Evaluate the accuracy
cv_results_1 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(.interactive = FALSE)
##################################
# Fitting CV results to our model:
##################################
## Refit to all data then forecast
prophet_fullfit <- cv_results_1 %>%
modeltime_refit(data = storeItemTrain)
prophet_preds_1 <- prophet_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=storeItemTest, by="date") %>%
select(id, sales)
## Save storeItem predictions
if(s==1 & i==1){
all_preds <- preds
}
else {
all_preds <- bind_rows(all_preds, preds)
}
}
}
library(modeltime) #Extensions of tidymodels to time series
library(timetk) #Some nice time series functions
library(gridExtra)
library(plotly)
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- data_train %>%
filter(store==s, item==i)
storeItemTest <- data_test %>%
filter(store==s, item==i)
## Fit storeItem models here
prophet_recipe <- recipe(rFormula, data = storeItemTrain) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(prophet_recipe) # preprocessing new data
baked_data_prophet <- bake(prepped_recipe, new_data = storeItemTrain)
cv_split_1 <- time_series_split(storeItemTrain, assess="3 months", cumulative = TRUE)
cv_split_1 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_model_1 <- prophet_reg() %>%
set_engine(engine = "prophet") %>%
fit(sales ~ date, data = training(cv_split_1))
cv_results_1 <- modeltime_calibrate(prophet_model_1,
new_data = testing(cv_split_1))
## Evaluate the accuracy
cv_results_1 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(.interactive = FALSE)
##################################
# Fitting CV results to our model:
##################################
## Refit to all data then forecast
prophet_fullfit <- cv_results_1 %>%
modeltime_refit(data = storeItemTrain)
prophet_preds_1 <- prophet_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=storeItemTest, by="date") %>%
select(id, sales)
## Save storeItem predictions
if(s==1 & i==1){
all_preds <- preds
}
else {
all_preds <- bind_rows(all_preds, preds)
}
}
}
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- data_train %>%
filter(store==s, item==i)
storeItemTest <- data_test %>%
filter(store==s, item==i)
## Fit storeItem models here
prophet_recipe <- recipe(rFormula, data = storeItemTrain) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(prophet_recipe) # preprocessing new data
baked_data_prophet <- bake(prepped_recipe, new_data = storeItemTrain)
cv_split_1 <- time_series_split(storeItemTrain, assess="3 months", cumulative = TRUE)
cv_split_1 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_model_1 <- prophet_reg() %>%
set_engine(engine = "prophet") %>%
fit(sales ~ date, data = training(cv_split_1))
cv_results_1 <- modeltime_calibrate(prophet_model_1,
new_data = testing(cv_split_1))
## Evaluate the accuracy
cv_results_1 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(.interactive = FALSE)
##################################
# Fitting CV results to our model:
##################################
## Refit to all data then forecast
prophet_fullfit <- cv_results_1 %>%
modeltime_refit(data = storeItemTrain)
prophet_preds_1 <- prophet_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=storeItemTest, by="date") %>%
select(id, sales)
## Save storeItem predictions
if(s==1 & i==1){
all_preds <- prophet_preds_1
}
else {
all_preds <- bind_rows(all_preds, prophet_preds_1)
}
}
}
vroom_write(all_preds, "./data/ItemDemand_preds.csv", delim = ",")
