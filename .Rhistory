burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu_ea <- 0.77
sigma2_ea <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu_ea), ylab="density", main=expression(pi(mu_ea~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu_ea <- 0.77
sigma2_ea <- 0.07^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu_ea), ylab="density", main=expression(pi(mu_ea~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
legend("topright", c("Posterior"))
# Posterior Dist of the difference between WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[WE]-mu[EA]), main="Posterior of "
+ expression(mu[WE]) + " - " + expression(mu[EA]))
# Posterior Dist of the difference between WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[WE]-mu[EA]), main=expression(~"Posterior of " ~mu[WE] ~" - " ~mu[EA]))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
#Given our data and prior knowledge, there is a 95% chance that the
#NYTimes bestseller Fiction goodreads ratings are between 0.3 points
#lower and 0.18 points higher than the nonfiction books
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
setwd("~/byu_fall_2023/Stat_348/STAT348/ItemDemand")
library(timetk)
library(gridExtra)
library(ggplot2)
data_train <- vroom("./data/train.csv")
library(timetk)
library(gridExtra)
library(ggplot2)
data_train <- vroom("./data/train.csv")
library(forecast)
storeItem <- data_train %>%
filter(store==9, item==36) %>%
select(date, sales)
rFormula <- sales ~ .
arima_recipe <- recipe(rFormula, data = storeItem) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(arima_recipe) # preprocessing new data
baked_data_arima <- bake(prepped_recipe, new_data = storeItem)
arima_model <- arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
install.packages('forecast')
install.packages("forecast")
library(forecast)
arima_model <- arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
library(forecast)
arima_model <- forecast::arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
install.packages('modeltime')
library(forecast)
library(modeltime)
storeItem <- data_train %>%
filter(store==9, item==36) %>%
select(date, sales)
rFormula <- sales ~ .
arima_recipe <- recipe(rFormula, data = storeItem) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe <- prep(arima_recipe) # preprocessing new data
baked_data_arima <- bake(prepped_recipe, new_data = storeItem)
arima_model <- forecast::arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
arima_model <- arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasona_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
arima_model <- arima_reg(seasonal_period=S,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
arima_wf <- workflow() %>%
add_recipe(arima_recipe) %>%
add_model(arima_model) %>%
fit(data=training(cv_split))
cv_split <- time_series_split(storeItem, assess="3 months", cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf <- workflow() %>%
add_recipe(arima_recipe) %>%
add_model(arima_model) %>%
fit(data=training(cv_split))
arima_model <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
cv_split <- time_series_split(storeItem, assess="3 months", cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf <- workflow() %>%
add_recipe(arima_recipe) %>%
add_model(arima_model) %>%
fit(data=training(cv_split))
cv_results <- modeltime_forecast(arima_model,
new_data = testing(cv_split))
cv_results <- modeltime_calibrate(arima_model,
new_data = testing(cv_split))
cv_results <- modeltime_calibrate(arima_wf,
new_data = testing(cv_split))
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
data_train <- vroom("./data/train.csv")
library(forecast)
library(modeltime)
storeItem1 <- data_train %>%
filter(store==9, item==36) %>%
select(date, sales)
rFormula <- sales ~ .
arima_recipe_1 <- recipe(rFormula, data = storeItem1) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe_1 <- prep(arima_recipe_1) # preprocessing new data
baked_data_arima1 <- bake(prepped_recipe_1, new_data = storeItem_1)
arima_model1 <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
cv_split_1 <- time_series_split(storeItem_1, assess="3 months", cumulative = TRUE)
cv_split_1 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf_1 <- workflow() %>%
add_recipe(arima_recipe_1) %>%
add_model(arima_model1) %>%
fit(data=training(cv_split_1))
cv_results_1 <- modeltime_calibrate(arima_wf_1,
new_data = testing(cv_split_1))
storeItem1 <- data_train %>%
filter(store==9, item==36) %>%
select(date, sales)
rFormula <- sales ~ .
arima_recipe_1 <- recipe(rFormula, data = storeItem1) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe_1 <- prep(arima_recipe_1) # preprocessing new data
baked_data_arima1 <- bake(prepped_recipe_1, new_data = storeItem1)
arima_model1 <- arima_reg(seasonal_period=365,
non_seasonal_ar=5, # default max p to tune
non_seasonal_ma=5, # default max q to tune
seasonal_ar=2, # default max P to tune
seasonal_ma=2, #default max Q to tune
non_seasonal_differences=2, # default max d to tune
seasonal_differences=2) %>% #default max D to tune
set_engine("auto_arima")
cv_split_1 <- time_series_split(storeItem1, assess="3 months", cumulative = TRUE)
cv_split_1 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf_1 <- workflow() %>%
add_recipe(arima_recipe_1) %>%
add_model(arima_model1) %>%
fit(data=training(cv_split_1))
cv_results_1 <- modeltime_calibrate(arima_wf_1,
new_data = testing(cv_split_1))
storeItem2 <- data_train %>%
filter(store==3, item==17) %>%
select(date, sales)
arima_recipe_2 <- recipe(rFormula, data = storeItem2) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
arima_recipe_2 <- recipe(rFormula, data = storeItem2) %>% # set model formula and dataset
step_date(date, features = c('dow', 'doy', 'week', 'month', 'year', 'decimal')) %>%
step_holiday(date) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))
prepped_recipe_2 <- prep(arima_recipe_2) # preprocessing new data
baked_data_arima2 <- bake(prepped_recipe_2, new_data = storeItem2)
cv_split_2 <- time_series_split(storeItem2, assess="3 months", cumulative = TRUE)
cv_split_2 %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf_2 <- workflow() %>%
add_recipe(arima_recipe_2) %>%
add_model(arima_model2) %>%
fit(data=training(cv_split_2))
arima_wf_2 <- workflow() %>%
add_recipe(arima_recipe_2) %>%
add_model(arima_model1) %>%
fit(data=training(cv_split_2))
cv_results_2 <- modeltime_calibrate(arima_wf_2,
new_data = testing(cv_split_2))
train_1 <- data_train %>% filter(store==9, item==36)
train_2 <- data_train %>% filter(store==3, item==17)
fig1 <- cv_results_1 %>%
modeltime_forecast(
new_data = testing(cv_split_1),
actual_data = train_1
) %>%
plot_modeltime_forecast(.interactive=TRUE)
fig1
## Evaluate the accuracy
cv_results_1 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(.interactive = FALSE)
## Refit to all data then forecast
arima_fullfit <- cv_results_1 %>%
modeltime_refit(data = train_1)
data_test <- vroom("./data/test.csv")
test_1 <- data_test %>% filter(store==9, item==36)
arima_preds_1 <- arima_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
arima_preds_1 <- arima_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test_1, by="date") %>%
select(id, sales)
fig3 <- arima_fullfit %>%
modeltime_forecast(h = "3 months", actual_data = train_1) %>%
plot_modeltime_forecast(.interactive=FALSE)
fig3
arima_preds_1
arima_fullfit
fig3 <- arima_fullfit %>%
modeltime_forecast(h = "3 months", actual_data = train_1) %>%
plot_modeltime_forecast(.interactive=FALSE)
fig3
fig3
fig2 <- cv_results_2 %>%
modeltime_forecast(
new_data = testing(cv_split_2),
actual_data = train_2
) %>%
plot_modeltime_forecast(.interactive=TRUE)
## Evaluate the accuracy
cv_results_2 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(.interactive = FALSE)
fig2
arima_fullfit_2 <- cv_results_2 %>%
modeltime_refit(data = train_2)
test_2 <- data_test %>% filter(store==3, item==17)
arima_preds_2 <- arima_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test_2, by="date") %>%
select(id, sales)
fig4 <- arima_fullfit_2 %>%
modeltime_forecast(h = "3 months", actual_data = train_2) %>%
plot_modeltime_forecast(.interactive=FALSE)
fig4
fig_grid_arima <- subplot(fig1, fig2, fig3, fig4, nrows = 2) %>%
layout(
title = 'Time Series Prediction',
annotations = list(
list(x = 0.2, y = 1, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 1, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE),
list(x = 0.2, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE)
)
)
library(modeltime) #Extensions of tidymodels to time series
library(timetk) #Some nice time series functions
library(gridExtra)
library(plotly)
fig_grid_arima <- subplot(fig1, fig2, fig3, fig4, nrows = 2) %>%
layout(
title = 'Time Series Prediction',
annotations = list(
list(x = 0.2, y = 1, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 1, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE),
list(x = 0.2, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE)
)
)
htmlwidgets::saveWidget(fig_grid_arima, "ts_arima_grid.html")
fig_grid_arima <- subplot(fig1, fig2, fig3, fig4, nrows = 2) %>%
layout(
title = 'Time Series Prediction: Arima Models',
annotations = list(
list(x = 0.2, y = 1, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 1, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE),
list(x = 0.2, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 9, Item 36', showarrow = FALSE),
list(x = 0.8, y = 0.45, xref = 'paper', yref = 'paper', text = 'Store 3, Item 17', showarrow = FALSE)
)
)
htmlwidgets::saveWidget(fig_grid_arima, "ts_arima_grid.html")
